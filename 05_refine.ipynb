{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: refine.html\n",
    "title: refine\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "import math\n",
    "from collections import defaultdict, OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from joblib import Parallel, delayed\n",
    "from operator import sub\n",
    "from statistics import mean\n",
    "from LAFITE.utils import loc_distance, Vividict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "@ dataclass\n",
    "class AttributeCollection:\n",
    "    start: int\n",
    "    end: int\n",
    "    count: float\n",
    "    polya_count: int = None\n",
    "    fsm: bool = False\n",
    "    polyaed: bool = False\n",
    "    as_site: list = None\n",
    "    apa_site: list = None\n",
    "    name: str = None\n",
    "    reference_id: list = None\n",
    "    rss_dis: int = math.inf\n",
    "    read_tag: str = None\n",
    "    processed: bool = False\n",
    "    chrand_ID: int = None\n",
    "    loci_ID: int = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "class SingleExonReadRefine:\n",
    "    def __init__(self, chrom, strand, chrand_collected_single_exon_read, chrand_ref_single_exon_trans, chrand_ref_mutple_exon_trans, chrand_three_prime_exon, min_single_exon_coverage, min_single_exon_len, rs_tolerance=24, coverage_cliff=0):\n",
    "        self.chrom = chrom\n",
    "        self.strand = strand\n",
    "        self.chrand_collected_single_exon_read = chrand_collected_single_exon_read\n",
    "        self.chrand_ref_single_exon_trans = chrand_ref_single_exon_trans\n",
    "        self.chrand_ref_mutple_exon_trans = chrand_ref_mutple_exon_trans\n",
    "        self.chrand_three_prime_exon = chrand_three_prime_exon\n",
    "        self.min_single_exon_coverage = min_single_exon_coverage\n",
    "        self.min_single_exon_len = min_single_exon_len\n",
    "        self.rs_tolerance = rs_tolerance\n",
    "        self.coverage_cliff = coverage_cliff\n",
    "\n",
    "    def single_exon_refine(self, subread, position, coverage, refined_single_exon_read, i):\n",
    "        if subread[1]-subread[0] >= self.min_single_exon_len:\n",
    "            subread_attribute = AttributeCollection(\n",
    "                position[subread[0]], position[subread[1]], mean(coverage[subread[0]:subread[1]]))\n",
    "            if self.chrand_ref_single_exon_trans:\n",
    "                overlap_ref = tuple(self.chrand_ref_single_exon_trans.find(\n",
    "                    (position[subread[0]], position[subread[1]])))\n",
    "                if overlap_ref:\n",
    "                    for ref_exon in overlap_ref:\n",
    "                        overlap = list(set(range(\n",
    "                            position[subread[0]], position[subread[1]]+1)).intersection(range(ref_exon[0], ref_exon[1]+1)))\n",
    "                        overlap.sort()\n",
    "                        if (overlap[-1] - overlap[0] + 1)/(ref_exon[1] - ref_exon[0] + 1) >= 0.5:\n",
    "                            subread_attribute.processed = True\n",
    "                            subread_attribute.read_tag = 'Keep_ref'\n",
    "                            break\n",
    "            if not subread_attribute.processed and self.chrand_three_prime_exon:\n",
    "                overlap_ref = tuple(self.chrand_three_prime_exon.find(\n",
    "                    (position[subread[0]], position[subread[1]])))\n",
    "                if overlap_ref:\n",
    "                    for ref_exon in overlap_ref:\n",
    "                        if (self.strand == '+' and position[subread[0]] > ref_exon[0] - self.rs_tolerance) or (self.strand == '-' and position[subread[1]] < ref_exon[1] + self.rs_tolerance):\n",
    "                            subread_attribute.processed = True\n",
    "                            break\n",
    "            if not subread_attribute.processed:\n",
    "                if subread_attribute.count >= self.min_single_exon_coverage:\n",
    "                    subread_attribute.processed = True\n",
    "                    subread_attribute.read_tag = 'Keep_coverage'\n",
    "            if subread_attribute.read_tag:\n",
    "                refined_single_exon_read[(\n",
    "                    position[subread[0]], position[subread[1]])] = subread_attribute\n",
    "        subread = [i+1, i+1]\n",
    "        return subread, refined_single_exon_read\n",
    "\n",
    "    def refine(self):\n",
    "        refined_single_exon_read = defaultdict(dict)\n",
    "        chrand_collected_single_exon_read = dict(OrderedDict(\n",
    "            sorted(self.chrand_collected_single_exon_read.items())))\n",
    "        position = list(chrand_collected_single_exon_read.keys())\n",
    "        coverage = list(chrand_collected_single_exon_read.values())\n",
    "        subread = [0, 0]\n",
    "        if not self.chrand_ref_mutple_exon_trans:\n",
    "            self.coverage_cliff = 0.9\n",
    "        for i in range(len(position)-1):\n",
    "            if position[i+1] == position[i]+1 and ((self.strand == '+' and coverage[i+1]/coverage[i] >= self.coverage_cliff) or (self.strand == '-' and coverage[i]/coverage[i+1] >= self.coverage_cliff)):\n",
    "                subread[1] = i+1\n",
    "                if i == len(position) - 2:\n",
    "                    subread, refined_single_exon_read = self.single_exon_refine(\n",
    "                        subread, position, coverage, refined_single_exon_read, i)\n",
    "            else:\n",
    "                subread, refined_single_exon_read = self.single_exon_refine(\n",
    "                    subread, position, coverage, refined_single_exon_read, i)\n",
    "        return self.chrom, self.strand, refined_single_exon_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "class MultiExonReadRefine():\n",
    "    def __init__(self, chrom, strand, chrand_processed_collected_multi_exon_read, chrand_ref_mutple_exon_trans, chrand_tss_dict, tss_cutoff, tes_cutoff, min_novel_trans_count, relative_fl_coverage=5, rs_tolerance=24, re_tolerance=50):\n",
    "        self.chrom = chrom\n",
    "        self.strand = strand\n",
    "        self.chrand_processed_collected_multi_exon_read = chrand_processed_collected_multi_exon_read\n",
    "        self.chrand_ref_mutple_exon_trans = chrand_ref_mutple_exon_trans\n",
    "        self.chrand_tss_dict = chrand_tss_dict\n",
    "        self.tss_cutoff = tss_cutoff\n",
    "        self.tes_cutoff = tes_cutoff\n",
    "        self.rs_tolerance = rs_tolerance\n",
    "        self.re_tolerance = re_tolerance\n",
    "        self.min_novel_trans_count = min_novel_trans_count\n",
    "        self.relative_fl_coverage = relative_fl_coverage\n",
    "\n",
    "    def trucated_reads_filtering(self, start, end, corrected_read_splicing, trans_structure_pool, refined_multi_exon_read, total_count):\n",
    "        for ref_iso_splicing in trans_structure_pool:\n",
    "            tmp_tag = None\n",
    "            if len(ref_iso_splicing) > len(corrected_read_splicing):\n",
    "                if set(corrected_read_splicing).issubset(set(ref_iso_splicing)):\n",
    "                    idx_start = ref_iso_splicing.index(\n",
    "                        corrected_read_splicing[0])\n",
    "                    idx_end = ref_iso_splicing.index(\n",
    "                        corrected_read_splicing[-1])\n",
    "                    if corrected_read_splicing == ref_iso_splicing[idx_start:idx_end+1]:\n",
    "                        if self.strand == '+' and idx_start > 0:\n",
    "                            if idx_end == len(ref_iso_splicing) - 1:\n",
    "                                if start >= ref_iso_splicing[idx_start-1] - self.rs_tolerance:\n",
    "                                    tmp_tag = 'Disqualify_Trucated_ISM'\n",
    "                            else:\n",
    "                                if start >= ref_iso_splicing[idx_start-1] - self.rs_tolerance and end <= ref_iso_splicing[idx_end+1] + self.re_tolerance:\n",
    "                                    tmp_tag = 'Disqualify_Trucated_ISM'\n",
    "\n",
    "                        elif self.strand == '-' and idx_end < len(ref_iso_splicing)-1:\n",
    "                            if idx_start == 0:\n",
    "                                if start <= ref_iso_splicing[idx_end+1] + self.rs_tolerance:\n",
    "                                    tmp_tag = 'Disqualify_Trucated_ISM'\n",
    "                            else:\n",
    "                                if start <= ref_iso_splicing[idx_end+1] + self.rs_tolerance and end >= ref_iso_splicing[idx_start-1] - self.rs_tolerance:\n",
    "                                    tmp_tag = 'Disqualify_Trucated_ISM'\n",
    "                    if tmp_tag:\n",
    "                        if ref_iso_splicing in refined_multi_exon_read:\n",
    "                            if total_count/refined_multi_exon_read[ref_iso_splicing].count >= self.relative_fl_coverage:\n",
    "                                tmp_tag = None\n",
    "                    if tmp_tag:\n",
    "                        break\n",
    "        return tmp_tag\n",
    "\n",
    "    def ISM_NIC_sorting(self, corrected_read_splicing, trans_structure_pool, rss_dis, total_count, start_pos=[], relative_intact=[], tmp_tag=None):\n",
    "        for ref_iso_splicing in trans_structure_pool:\n",
    "            if len(ref_iso_splicing) > len(corrected_read_splicing):\n",
    "                if set(corrected_read_splicing).issubset(set(ref_iso_splicing)):\n",
    "                    idx_start = ref_iso_splicing.index(\n",
    "                        corrected_read_splicing[0])\n",
    "                    idx_end = ref_iso_splicing.index(\n",
    "                        corrected_read_splicing[-1])\n",
    "                    if (self.strand == '+' and idx_start == 0) or (self.strand == '-' and idx_end == len(ref_iso_splicing)-1):\n",
    "                        start_pos.append(1)\n",
    "                    else:\n",
    "                        start_pos.append(0)\n",
    "                    ref_exon_num = (len(ref_iso_splicing)+2)/2\n",
    "                    if self.strand == '+':\n",
    "                        relative_intact.append(\n",
    "                            (ref_exon_num - idx_start/2)/ref_exon_num)\n",
    "                    else:\n",
    "                        relative_intact.append(((idx_end+3)/2)/ref_exon_num)\n",
    "        if start_pos:\n",
    "            if all(start_pos) or rss_dis < self.tss_cutoff:\n",
    "                tmp_tag = 'Subset'\n",
    "            elif total_count >= self.min_novel_trans_count and mean(relative_intact) > 0.5:\n",
    "                tmp_tag = 'Subset_coverage'\n",
    "            else:\n",
    "                tmp_tag = 'Disqualify_ISM_NIC'\n",
    "\n",
    "        return tmp_tag\n",
    "\n",
    "    def closest_ref_trans(self, corrected_read_splicing, trans_structure_pool):\n",
    "        \"\"\" return the closest reference transcripts for the input read\n",
    "        \"\"\"\n",
    "        inter_SJ = ()\n",
    "        cmp_trans = ()\n",
    "        for ref_iso_splicing in trans_structure_pool:\n",
    "            if self.strand == '-':\n",
    "                ref_iso_splicing = tuple(reversed(ref_iso_splicing))\n",
    "                corrected_read_splicing = tuple(\n",
    "                    reversed(corrected_read_splicing))\n",
    "\n",
    "            tmp_inter = list(set(ref_iso_splicing).intersection(\n",
    "                set(corrected_read_splicing)))\n",
    "            tmp_inter.sort(reverse=True)\n",
    "            if len(tmp_inter) > len(inter_SJ):\n",
    "                inter_SJ, cmp_trans = tmp_inter, ref_iso_splicing\n",
    "            elif len(tmp_inter) == len(inter_SJ) and len(tmp_inter) > 0:\n",
    "                index1 = [i for i, val in enumerate(\n",
    "                    cmp_trans) if val in inter_SJ]\n",
    "                index2 = [i for i, val in enumerate(\n",
    "                    ref_iso_splicing) if val in tmp_inter]\n",
    "                index_sub = list(map(sub, index1, index2))\n",
    "                if any(index_sub):\n",
    "                    if index_sub[next((i for i, x in enumerate(index_sub) if x != 0), None)] > 0:\n",
    "                        inter_SJ = tmp_inter\n",
    "                        cmp_trans = ref_iso_splicing\n",
    "                elif len(ref_iso_splicing) < len(cmp_trans):\n",
    "                    inter_SJ = tmp_inter\n",
    "                    cmp_trans = ref_iso_splicing\n",
    "\n",
    "        if self.strand == '-':\n",
    "            cmp_trans = tuple(reversed(cmp_trans))\n",
    "\n",
    "        return len(inter_SJ), cmp_trans\n",
    "\n",
    "    def secondary_refine(self, corrected_read_splicing, read_attribute, refined_multi_exon_read, loci_idx):\n",
    "        if 'Disqualify' not in read_attribute.read_tag:\n",
    "            if read_attribute.read_tag != 'Reference':\n",
    "                for ref_iso_splicing in refined_multi_exon_read:\n",
    "                    if set(corrected_read_splicing).issubset(set(ref_iso_splicing)):\n",
    "                        idx_start = ref_iso_splicing.index(\n",
    "                            corrected_read_splicing[0])\n",
    "                        idx_end = ref_iso_splicing.index(\n",
    "                            corrected_read_splicing[-1])\n",
    "                        if corrected_read_splicing == ref_iso_splicing[idx_start:idx_end+1]:\n",
    "                            if (self.strand == '+' and idx_end == len(ref_iso_splicing) - 1) or (self.strand == '-' and idx_start == 0):\n",
    "                                if (self.strand == \"+\" and read_attribute.start >= ref_iso_splicing[idx_start-1] - self.rs_tolerance) or (self.strand == \"-\" and read_attribute.start <= ref_iso_splicing[idx_end+1] + self.rs_tolerance):\n",
    "                                    if read_attribute.count/refined_multi_exon_read[ref_iso_splicing].count < self.relative_fl_coverage:\n",
    "                                        read_attribute.read_tag = 'Disqualify_merged'\n",
    "                                        break\n",
    "\n",
    "        if 'Disqualify' not in read_attribute.read_tag:\n",
    "            len_inter_SJ, cmp_trans = self.closest_ref_trans(\n",
    "                corrected_read_splicing, refined_multi_exon_read)\n",
    "            if len_inter_SJ > 0:\n",
    "                read_attribute.chrand_ID = refined_multi_exon_read[cmp_trans].chrand_ID\n",
    "            else:\n",
    "                loci_idx += 1\n",
    "                read_attribute.chrand_ID = loci_idx\n",
    "\n",
    "            refined_multi_exon_read[corrected_read_splicing] = read_attribute\n",
    "\n",
    "        return refined_multi_exon_read, read_attribute, loci_idx\n",
    "\n",
    "    def main_refine(self, loci_idx=0):\n",
    "        refined_multi_exon_read = defaultdict(dict)\n",
    "        refine_log = defaultdict(dict)\n",
    "        for corrected_read_splicing, read_info in self.chrand_processed_collected_multi_exon_read.items():\n",
    "            read_attribute = AttributeCollection(*read_info)\n",
    "            # print(read_info)\n",
    "            trans_structure_pool = self.chrand_ref_mutple_exon_trans\n",
    "\n",
    "            # calculate the distance between TSS of the collapsed read and nearest reference TSS\n",
    "            read_attribute.rss_dis, _ = loc_distance(\n",
    "                self.chrand_tss_dict, read_attribute.start)\n",
    "\n",
    "            # check full splicing match reads whose rss and exactly matched reference transcripts\n",
    "            if read_attribute.fsm:\n",
    "                if any(abs(x - trans_structure_pool[corrected_read_splicing][0]) <= self.tss_cutoff for x in read_attribute.as_site):\n",
    "                    if read_attribute.polyaed:\n",
    "                        read_attribute.read_tag = 'Reference'\n",
    "                        read_attribute.processed = True\n",
    "                    elif any(abs(x - trans_structure_pool[corrected_read_splicing][1]) <= self.tes_cutoff for x in read_attribute.apa_site):\n",
    "                        read_attribute.read_tag = 'Reference'\n",
    "                        read_attribute.processed = True\n",
    "\n",
    "            if not read_attribute.processed:\n",
    "                # check if read is trucated\n",
    "                if trans_structure_pool:\n",
    "                    read_tag = self.trucated_reads_filtering(\n",
    "                        read_attribute.start, read_attribute.end, corrected_read_splicing, trans_structure_pool, refined_multi_exon_read, read_attribute.count)\n",
    "                    if read_tag:\n",
    "                        read_attribute.read_tag = read_tag\n",
    "                        read_attribute.processed = True\n",
    "\n",
    "                if not read_attribute.processed and read_attribute.fsm:\n",
    "                    read_attribute.read_tag = 'Reference'\n",
    "                    read_attribute.processed = True\n",
    "\n",
    "                if not read_attribute.processed and not read_attribute.polyaed:\n",
    "                    read_attribute.read_tag = 'Disqualify_No_ployA'\n",
    "                    read_attribute.processed = True\n",
    "\n",
    "                elif not read_attribute.processed:\n",
    "                    if trans_structure_pool:\n",
    "                        read_tag = self.ISM_NIC_sorting(\n",
    "                            corrected_read_splicing, trans_structure_pool, read_attribute.rss_dis, read_attribute.count, start_pos=[], tmp_tag=None)\n",
    "                        if read_tag:\n",
    "                            read_attribute.read_tag = read_tag\n",
    "                            read_attribute.processed = True\n",
    "\n",
    "                    if not read_attribute.processed:\n",
    "                        len_inter_SJ, cmp_trans = self.closest_ref_trans(\n",
    "                            corrected_read_splicing, trans_structure_pool)\n",
    "                        read_attribute.processed = True\n",
    "                        if len_inter_SJ > 0:\n",
    "                            if (cmp_trans[0] == corrected_read_splicing[0]) or set(cmp_trans).issubset(set(corrected_read_splicing)) or read_attribute.rss_dis <= self.tss_cutoff:\n",
    "                                read_attribute.read_tag = 'Similar'\n",
    "                            else:\n",
    "                                read_attribute.read_tag = 'Disqualify_NNC'\n",
    "                        else:\n",
    "                            if read_attribute.rss_dis <= self.tss_cutoff or read_attribute.count >= self.min_novel_trans_count:\n",
    "                                read_attribute.read_tag = 'Novel_loci'\n",
    "                            else:\n",
    "                                read_attribute.read_tag = \"Disqualify_Other\"\n",
    "\n",
    "            refined_multi_exon_read, read_attribute, loci_idx = self.secondary_refine(\n",
    "                corrected_read_splicing, read_attribute, refined_multi_exon_read, loci_idx)\n",
    "            refine_log[corrected_read_splicing] = read_attribute\n",
    "        return self.chrom, self.strand, refined_multi_exon_read, refine_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "class RefineWrapper:\n",
    "    def __init__(self, processed_collected_multi_exon_read, collected_single_exon_read, ref_mutple_exon_trans, ref_single_exon_trans, three_prime_exon, tss_dict, tss_cutoff, tes_cutoff, min_novel_trans_count, min_single_exon_coverage, min_single_exon_len, thread, tmp_dir):\n",
    "        self.processed_collected_multi_exon_read = processed_collected_multi_exon_read\n",
    "        self.collected_single_exon_read = collected_single_exon_read\n",
    "        self.ref_mutple_exon_trans = ref_mutple_exon_trans\n",
    "        self.ref_single_exon_trans = ref_single_exon_trans\n",
    "        self.three_prime_exon = three_prime_exon\n",
    "        self.tss_dict = tss_dict\n",
    "        self.tss_cutoff = tss_cutoff\n",
    "        self.tes_cutoff = tes_cutoff\n",
    "        self.min_novel_trans_count = min_novel_trans_count\n",
    "        self.min_single_exon_coverage = min_single_exon_coverage\n",
    "        self.min_single_exon_len = min_single_exon_len\n",
    "        self.thread = thread\n",
    "        self.tmp_dir = tmp_dir\n",
    "\n",
    "    def run1(self):\n",
    "        multi_precompute_list = []\n",
    "        for (chrom, strand), chrand_processed_collected_multi_exon_read in self.processed_collected_multi_exon_read.items():\n",
    "            multi_precompute_list.append(MultiExonReadRefine(chrom, strand, chrand_processed_collected_multi_exon_read, self.ref_mutple_exon_trans[(\n",
    "                chrom, strand)], self.tss_dict[(chrom, strand)], self.tss_cutoff, self.tes_cutoff, self.min_novel_trans_count))\n",
    "        with Parallel(n_jobs=self.thread) as parallel:\n",
    "            multi_exon_results = parallel(delayed(lambda x: x.main_refine())(\n",
    "                job) for job in multi_precompute_list)\n",
    "\n",
    "        return multi_exon_results\n",
    "\n",
    "    def run2(self):\n",
    "        single_precompute_list = []\n",
    "        for (chrom, strand), chrand_collected_single_exon_read in self.collected_single_exon_read.items():\n",
    "            single_precompute_list.append(SingleExonReadRefine(chrom, strand, chrand_collected_single_exon_read, self.ref_single_exon_trans[(\n",
    "                chrom, strand)], self.ref_mutple_exon_trans[(chrom, strand)], self.three_prime_exon[(chrom, strand)], self.min_single_exon_coverage, self.min_single_exon_len))\n",
    "        with Parallel(n_jobs=self.thread) as parallel:\n",
    "            single_exon_results = parallel(delayed(lambda x: x.refine())(\n",
    "                job) for job in single_precompute_list)\n",
    "\n",
    "        return single_exon_results\n",
    "\n",
    "    def result_collection(self):\n",
    "        multi_exon_results = self.run1()\n",
    "        single_exon_results = self.run2()\n",
    "        collected_refined_isoforms = Vividict()\n",
    "        path_to_refine_log = f'{self.tmp_dir}/refine.log'\n",
    "        for result in sorted(multi_exon_results):\n",
    "            chrom, strand, refined_multi_exon_read, refine_log = result\n",
    "            for corrected_read_splicing, read_attribute in refined_multi_exon_read.items():\n",
    "                if strand == '-':\n",
    "                    corrected_read_splicing = (\n",
    "                        read_attribute.end,) + corrected_read_splicing + (read_attribute.start,)\n",
    "                else:\n",
    "                    corrected_read_splicing = (\n",
    "                        read_attribute.start,) + corrected_read_splicing + (read_attribute.end,)\n",
    "                collected_refined_isoforms[(\n",
    "                    chrom, strand)][corrected_read_splicing] = read_attribute\n",
    "            with open(path_to_refine_log, 'a') as flog:\n",
    "                for corrected_read_splicing, read_attribute in refine_log.items():\n",
    "                    corrected_read_splicing = ','.join(\n",
    "                        [str(s) for s in corrected_read_splicing])\n",
    "                    read_attribute = read_attribute.__dict__\n",
    "                    read_attribute = '\\t'.join('{}: {}'.format(\n",
    "                        key, str(value)) for key, value in read_attribute.items())\n",
    "                    flog.write(\n",
    "                        f'{corrected_read_splicing}\\t{read_attribute}\\n')\n",
    "        for result in single_exon_results:\n",
    "            chrom, strand, refined_single_exon_read = result\n",
    "            for corrected_read_splicing, read_attribute in refined_single_exon_read.items():\n",
    "                collected_refined_isoforms[(\n",
    "                    chrom, strand)][corrected_read_splicing] = read_attribute\n",
    "\n",
    "        collected_refined_isoforms = dict(\n",
    "            sorted(collected_refined_isoforms.items()))\n",
    "\n",
    "        return collected_refined_isoforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
